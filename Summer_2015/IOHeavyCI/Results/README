Run 1 -- Didn't take into account the size of the file that we were reading. 
Run 2 -- Recorded read velocity, but didn't have the files trimmed to the same size. 
Run 3 -- No write to disk going on, all files are 2 GB.
Run 4 -- Writing output from /dev/urandom to disk while doing the read. The write speed was ~ 10 MB/s.
Run 5 -- We read from cache, the files are loaded into ram first then we do the reads. There was no writing to disk (that I controlled). 
Run 6 -- Read comes from the Hadoop FUSE mounted filesystem. All servers which hosted hadoop blocks had their cache cleared before the read.
			This run ended up breaking because of trouble dropping the cache of so many machines at once. We only got 2 data points.
Run 7 -- Read comes from Hadoop FUSE mounted filesystem, but there is no attempt made at cache dropping.
Run 8 -- Read through HDFS C API, no attempt at dropping caches, no writes
Run 9 -- Read on local disk (cabinet-8-8-0, all others were on UAF-4) with each file on it's own disk. I.E. even when multiple files were being read
			there was only a single file being read from any disk at once. This is more a test of the bus speed than anything else. 
Run 10 -- Another trial equivalent to run 9. 
Run 11 -- Another trial equivalent to run 9.